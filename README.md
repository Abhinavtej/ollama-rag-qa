### **ğŸ“– RAG-based Q&A with Ollama & Streamlit**  

A **Retrieval-Augmented Generation (RAG) system** that allows you to **upload PDFs, retrieve relevant chunks, and generate AI-powered answers using Llama 3.2 via Ollama**â€”all running locally! ğŸš€  

---

## **ğŸ“Œ Features**  
âœ… **PDF Upload & Storage** ğŸ“‚  
âœ… **Text Chunking & FAISS Vector Search** ğŸ”  
âœ… **Retrieve Relevant Information from Documents** ğŸ§©  
âœ… **Local AI-Powered Q&A with Llama 3.2 (Ollama)** ğŸ¤–  
âœ… **Simple UI with Streamlit** ğŸ›ï¸  

---

## **ğŸ› ï¸ Tech Stack**  
- **Python** ğŸ  
- **LangChain** for RAG-based retrieval  
- **FAISS** for vector embeddings  
- **Ollama** for running Llama 3.2 locally  
- **Streamlit** for a lightweight UI  

---

## **ğŸš€ Installation & Setup**  

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/abhinavtej/ollama-rag-qa.git
cd ollama-rag-qa
```

### **2ï¸âƒ£ Create a Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate     # On Windows
```

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### **4ï¸âƒ£ Run Ollama Server**
Ensure **Ollama** is installed:  
```bash
ollama serve &
```

### **5ï¸âƒ£ Start the Streamlit App**
```bash
streamlit run app.py
```

---

## **ğŸ“‚ Project Structure**
```
ollama_rag_qa/
â”‚â”€â”€ venv/                  # Virtual environment
â”‚â”€â”€ uploads/               # Folder to store uploaded PDFs
â”‚â”€â”€ app.py                 # Streamlit frontend
â”‚â”€â”€ rag_qa.py              # Backend processing
â”‚â”€â”€ config.py              # Configuration settings
â”‚â”€â”€ requirements.txt       # Dependencies
â”‚â”€â”€ README.md              # Documentation
â”‚â”€â”€ .gitignore             # Ignore unnecessary files
```

---

## **ğŸ“Œ Usage Guide**
1ï¸âƒ£ **Upload a PDF** via the Streamlit UI.  
2ï¸âƒ£ **Ask a question** related to the document.  
3ï¸âƒ£ The system **retrieves relevant sections** from the PDF.  
4ï¸âƒ£ **Llama 3.2** generates an answer based on retrieved context.  

---

ğŸ“¢ **Would love to hear your thoughts & suggestions!** Drop a comment or contribute via PRs. ğŸš€  

---

### **ğŸ”— Connect with Me**
ğŸ“§ Mail[mailto:abhinavtejreddy.j@gmail.com]  
ğŸ”— Linkedin[www.linkedin.com/in/abhinavtej]  
ğŸ™ Github[www.github.com/abhinavtej]  

---
